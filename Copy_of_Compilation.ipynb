{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Compilation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "####### XGBoost Forecasting #######"
      ],
      "metadata": {
        "id": "aUm464e88aQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6aWY8xm6aUU"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgbm\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
        "matplotlib.rcParams['axes.grid'] = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def mape(y_pred, y_train):\n",
        "    ape = np.abs((y_pred - y_train) / y_train)\n",
        "    ape[~np.isfinite(ape)] = 1\n",
        "    return np.mean(ape)"
      ],
      "metadata": {
        "id": "V7PovsFY6qp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Train_data.xlsx')\n",
        "df[datetime(2020, 4, 1, 0, 0)] = (df[datetime(2020, 3, 1, 0, 0)] + df[datetime(2020, 5, 1, 0, 0)])/2\n",
        "df = df.melt(id_vars=['Warehouse id', 'Region', 'SKU id'], var_name='date', value_name='sales')\n",
        "df['SKU id'] = df['SKU id'].str.extract('(\\d+)',expand = False).astype(int)\n",
        "df.rename(columns={'SKU id':'store_id'}, inplace=True)\n",
        "df = pd.concat([df,pd.get_dummies(df['Region'])], axis = 1)\n",
        "df.drop('Region', axis = 1, inplace = True)\n",
        "df['Year'] = df['date'].apply(lambda time: time.year)\n",
        "df['Month'] = df['date'].apply(lambda time: time.month)\n",
        "df.sort_values(['Warehouse id','store_id']).drop('Warehouse id', axis = 1)\n",
        "df['isFest'] = df['date'].apply(lambda x: x.month == 3 or x.month == 5 or x.month == 11 or x.month == 12).astype(int)\n",
        "df['isSum'] =  df['date'].apply(lambda x: x.month >= 4 and x.month<=7).astype(int)\n",
        "df['isWin'] =  df['date'].apply(lambda x: x.month == 10 or x.month == 11 or x.month == 12 or x.month == 1 or x.month == 2).astype(int)\n",
        "df = df.sort_values(['Warehouse id','store_id'])#.drop('Warehouse id', axis = 1)\n",
        "df['next'] = df.groupby(['store_id', 'Warehouse id'])['sales'].shift(-1)\n",
        "df['next'] = df['next'].fillna(df['next'].median())\n",
        "# df['next'].fillna(df.groupby(['store_id', 'Warehouse id'])['sales'].median())\n",
        "# df.head()\n",
        "a = df.groupby(['store_id', 'Warehouse id'])['sales'].rolling(6).mean()\n",
        "df['ra'] = a.fillna(a.mean()).reset_index(level=0, drop=True).reset_index(level=0, drop=True)\n",
        "df['ewm'] = df.groupby(['store_id', 'Warehouse id'])['sales'].ewm(alpha = 0.2).mean().reset_index(level=0, drop=True).reset_index(level = 0, drop = True)\n",
        "df.fillna(df.median(), inplace = True)\n",
        "\n",
        "split_pt = '2020-10-01'\n",
        "train_data = df[df['date'] <= split_pt]\n",
        "test_data = df[df['date'] > split_pt]\n",
        "# train_data\n",
        "\n",
        "train_data.drop(['Warehouse id', 'date', 'store_id'], axis = 1, inplace = True)\n",
        "test_data.drop(['Warehouse id', 'date', 'store_id'], axis = 1, inplace = True)\n",
        "\n",
        "X = train_data.drop(['next', 'Month', 'Year'], axis = 1)\n",
        "# X = train_data.drop(['sales'], axis = 1)\n",
        "y = train_data['next']\n",
        "model = XGBRegressor(n_estimators=1000)\n",
        "model.fit(X, y)\n",
        "yhat = model.predict(test_data.drop(['next', 'Month', 'Year'], axis = 1))\n",
        "test_data['pred next'] = yhat\n",
        "print(mape(yhat, test_data['next'])\n",
        "\n",
        "mar = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 3)]\n",
        "apr = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 4)]\n",
        "may = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 5)]\n",
        "\n",
        "print(mape(mar['pred next'], mar['next']))\n",
        "print(mape(apr['pred next'], apr['next']))\n",
        "print(mape(may['pred next'], may['next']))"
      ],
      "metadata": {
        "id": "m8pwYpe16sQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### LightBGM Forecasting #######"
      ],
      "metadata": {
        "id": "9w_gw2nR8gjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Train_data.xlsx')\n",
        "df[datetime(2020, 4, 1, 0, 0)] = (df[datetime(2020, 3, 1, 0, 0)] + df[datetime(2020, 5, 1, 0, 0)])/2\n",
        "df = df.melt(id_vars=['Warehouse id', 'Region', 'SKU id'], var_name='date', value_name='sales')\n",
        "df['SKU id'] = df['SKU id'].str.extract('(\\d+)',expand = False).astype(int)\n",
        "df.rename(columns={'SKU id':'store_id'}, inplace=True)\n",
        "df = pd.concat([df,pd.get_dummies(df['Region'])], axis = 1)\n",
        "df.drop('Region', axis = 1, inplace = True)\n",
        "df['Year'] = df['date'].apply(lambda time: time.year)\n",
        "df['Month'] = df['date'].apply(lambda time: time.month)\n",
        "df.sort_values(['Warehouse id','store_id']).drop('Warehouse id', axis = 1)\n",
        "df['isFest'] = df['date'].apply(lambda x: x.month == 3 or x.month == 5 or x.month == 11 or x.month == 12).astype(int)\n",
        "df['isSum'] =  df['date'].apply(lambda x: x.month >= 4 and x.month<=7).astype(int)\n",
        "df['isWin'] =  df['date'].apply(lambda x: x.month == 10 or x.month == 11 or x.month == 12 or x.month == 1 or x.month == 2).astype(int)\n",
        "df = df.sort_values(['Warehouse id','store_id'])#.drop('Warehouse id', axis = 1)\n",
        "df['next'] = df.groupby(['store_id', 'Warehouse id'])['sales'].shift(-1)\n",
        "df['next'] = df['next'].fillna(df['next'].median())\n",
        "# df['next'].fillna(df.groupby(['store_id', 'Warehouse id'])['sales'].median())\n",
        "# df.head()\n",
        "a = df.groupby(['store_id', 'Warehouse id'])['sales'].rolling(6).mean()\n",
        "df['ra'] = a.fillna(a.mean()).reset_index(level=0, drop=True).reset_index(level=0, drop=True)\n",
        "df['ewm'] = df.groupby(['store_id', 'Warehouse id'])['sales'].ewm(alpha = 0.2).mean().reset_index(level=0, drop=True).reset_index(level = 0, drop = True)\n",
        "df.fillna(df.median(), inplace = True)\n",
        "\n",
        "split_pt = '2020-10-01'\n",
        "train_data = df[df['date'] <= split_pt]\n",
        "test_data = df[df['date'] > split_pt]\n",
        "# train_data\n",
        "\n",
        "train_data.drop(['Warehouse id', 'date', 'store_id'], axis = 1, inplace = True)\n",
        "test_data.drop(['Warehouse id', 'date', 'store_id'], axis = 1, inplace = True)\n",
        "\n",
        "X = train_data.drop(['next', 'Month', 'Year'], axis = 1)\n",
        "# X = train_data.drop(['sales'], axis = 1)\n",
        "y = train_data['next']\n",
        "model = lgbm.LGBMRegressor(n_estimators=1000)\n",
        "model.fit(X, y)\n",
        "yhat = model.predict(test_data.drop(['next', 'Month', 'Year'], axis = 1))\n",
        "test_data['pred next'] = yhat\n",
        "print(mape(yhat, test_data['next'])\n",
        "\n",
        "mar = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 3)]\n",
        "apr = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 4)]\n",
        "may = test_data[(test_data['Year'] == 2021) & (test_data['Month'] == 5)]\n",
        "\n",
        "print(mape(mar['pred next'], mar['next']))\n",
        "print(mape(apr['pred next'], apr['next']))\n",
        "print(mape(may['pred next'], may['next']))"
      ],
      "metadata": {
        "id": "JsmyyfbI8k79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### RandomForestRegressor ######"
      ],
      "metadata": {
        "id": "UJydmL1J9RRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "data = pd.read_excel('Train_data.xlsx')\n",
        "# data.head(6)\n",
        "data_grouped_by_mean = data.groupby(by = 'Warehouse id').mean()\n",
        "data_grouped_by_mean\n",
        "data_melt = data.melt(id_vars=['Warehouse id','Region','SKU id'],var_name = 'month',value_name = 'sales')\n",
        "# data.head()\n",
        "# data_melt = data_melt.sort_values(['month','SKU id'])\n",
        "data_melt.head(6)\n",
        "data_melt['SKU id'] = data_melt['SKU id'].str.extract('(\\d+)',expand = False).astype(int)\n",
        "# data_melt['Warehouse id'] = data_melt['Warehouse id'].str.extract('(\\d+)',expand = False).astype(int)\n",
        "# data_melt = \n",
        "data_melt = data_melt.sort_values(['SKU id','Region','month'])\n",
        "data_melt.head()\n",
        "data_melt['sales next'] = data_melt.groupby(['SKU id','Region'])['sales'].shift(-1)\n",
        "data_melt['sales prev'] = data_melt.groupby(['SKU id','Region'])['sales'].shift(1)\n",
        "data_melt['sales diff'] = data_melt.groupby(['SKU id','Region'])['sales'].diff(1)\n",
        "# data_melt[(data_melt['SKU id'] == 1) & (data_melt['Region'] == 'NORTH')].head(10)\n",
        "data_melt.head(10)\n",
        "data_melt['mean sales 4'] = data_melt.groupby('SKU id')['sales'].rolling(window = 4,center = True).mean().reset_index(level = 0,drop = True)  \n",
        "data_melt['exp mean sales 4'] = data_melt.groupby('SKU id')['sales'].ewm(span=4).mean().reset_index(level = 0,drop = True)  \n",
        "data_melt['year'] =data_melt['month'].apply(lambda x:x.year).astype(int)\n",
        "data_melt.head()\n",
        "data_melt['is Festival Month'] = data_melt['month'].apply(lambda x: x.month == 1 or x.month == 3 or x.month == 11 or x.month == 12 ).astype(int)\n",
        "data_melt['is Summer'] = data_melt['month'].apply(lambda x: x.month == 5 or x.month == 6 or x.month == 7 or x.month == 8 ).astype(int)\n",
        "data_melt['is Winter'] = data_melt['month'].apply(lambda x: x.month == 11 or x.month == 12 or x.month == 1 or x.month == 2 ).astype(int)\n",
        "data_melt.head(10)\n",
        "data_melt.fillna(data_melt.mean(),inplace = True)\n",
        "# warehouse_encode = pd.get_dummies(data_melt['Warehouse id'], prefix = 'Warehouse')\n",
        "# warehouse_encode.head()\n",
        "region_encode = pd.get_dummies(data_melt['Region'],prefix = 'Region')\n",
        "data_melt.drop(labels = ['Warehouse id'],axis =1 ,inplace = True)\n",
        "encoded = pd.concat([region_encode,data_melt],axis = 1)\n",
        "encoded.reset_index(drop = True,inplace = True)\n",
        "encoded.head() \n",
        "\n",
        "split_point = '2021-01-01'\n",
        "train = encoded[encoded['month'] < split_point].copy()\n",
        "test = encoded[encoded['month'] >= split_point].copy()\n",
        "# train.rest_index(inplace = True)\n",
        "len(test)\n",
        "# train.head()\n",
        "\n",
        "train['month'] = train['month'].apply(lambda x:x.month).astype(int)\n",
        "test['month'] = test['month'].apply(lambda x:x.month).astype(int)\n",
        "# test['month'] = test['month'].apply(lambda x:x.month).astype(int)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators = 100,random_state = 0)\n",
        "Xtr = train.drop(['sales next','Region'],axis = 1)\n",
        "Ytr = train['sales next']\n",
        "# Xtr = train.drop(['sales','Region','sales diff'],axis = 1)\n",
        "# Ytr = train['sales']\n",
        "model.fit(Xtr,Ytr)\n",
        "\n",
        "Ytrue = test['sales next']\n",
        "Ypred = model.predict(test.drop(['sales next','Region'],axis = 1))\n",
        "# Ytrue = test['sales']\n",
        "# Ypred = model.predict(test.drop(['sales','Region','sales diff'],axis = 1))\n",
        "print(mape(Ytrue,Ypred))\n",
        "\n",
        "test['pred'] = Ypred\n",
        "# plt.plot(test['sales next'])\n",
        "# plt.plot(test['pred'])\n",
        "test.reset_index(inplace = True)\n",
        "test.sort_values(['SKU id'],inplace = True)\n",
        "test1 = test[test['month'] == 1 ]\n",
        "test2 = test[test['month'] == 2 ]\n",
        "test3 = test[test['month'] == 3]\n",
        "test4 = test[test['month'] == 4]\n",
        "test1.reset_index(inplace = True)\n",
        "test2.reset_index(inplace = True)\n",
        "test3.reset_index(inplace = True)\n",
        "test4.reset_index(inplace = True)"
      ],
      "metadata": {
        "id": "2aSpL4La9WH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### DeepAR Pytorch-Forecasting #######"
      ],
      "metadata": {
        "id": "we3Xe91f8Umt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-forecasting\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
        "matplotlib.rcParams['axes.grid'] = False\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ],
      "metadata": {
        "id": "rJsS-Zfy70uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/Train_data.xlsx')\n",
        "# df.drop(datetime(2020,4,1,0,0), axis=1, inplace=True)\n",
        "df = df.melt(id_vars=['Warehouse id', 'Region', 'SKU id'], var_name='date', value_name='sales')\n",
        "# df['SKU id'] = df['SKU id'].str.extract('(\\d+)',expand = False).astype(int)\n",
        "# df.rename(columns={'SKU id':'store_id'}, inplace=True)\n",
        "# df.head()\n",
        "\n",
        "# Feature Engineering\n",
        "df['sales previous month'] = df.groupby(['Region', 'SKU id'])['sales'].shift(1)\n",
        "df['diff sales 1'] = df.groupby(['Region', 'SKU id'])['sales'].diff(1)\n",
        "df.sort_values(['Region', 'SKU id', 'date'], inplace=True)\n",
        "df['rolling mean sales 4'] = df.groupby(['Warehouse id', 'SKU id'])['sales'].rolling(4).mean().reset_index(level=0, drop=True).reset_index(level=0, drop=True)\n",
        "df['exp mean'] = df.groupby(['Warehouse id', 'SKU id'])['sales'].ewm(alpha=0.2).mean().reset_index(level=0, drop=True).reset_index(level=0, drop=True)\n",
        "\n",
        "df['isHoli'] = df['date'].apply(lambda x: x.month == 3).astype(int)\n",
        "df['isDiwali'] = df['date'].apply(lambda x: x.month == 10).astype(int)\n",
        "df['isSankranti'] = df['date'].apply(lambda x: x.month == 1).astype(int)\n",
        "df['isChristmas'] = df['date'].apply(lambda x: x.month == 12).astype(int)\n",
        "df['isSummer'] = df['date'].apply(lambda x: x.month == 4 or x.month == 5 or x.month == 6).astype(int)\n",
        "df['isWinter'] = df['date'].apply(lambda x: x.month == 11 or x.month == 12 or x.month == 1 or x.month == 2).astype(int)\n",
        "\n",
        "df['isHoli'] = df['isHoli'].astype(str).astype(\"category\")\n",
        "df['isDiwali'] = df['isDiwali'].astype(str).astype(\"category\")\n",
        "df['isSankranti'] = df['isSankranti'].astype(str).astype(\"category\")\n",
        "df['isChristmas'] = df['isChristmas'].astype(str).astype(\"category\")\n",
        "df['isSummer'] = df.isSummer.astype(str).astype(\"category\")\n",
        "df['isWinter'] = df.isWinter.astype(str).astype(\"category\")\n",
        "\n",
        "df[\"month\"] = df.date.dt.month.astype(str).astype(\"category\")\n",
        "\n",
        "df.sort_values(['Warehouse id', 'SKU id', 'date'], inplace=True)\n",
        "\n",
        "df[\"time_idx\"] = df[\"date\"].dt.year * 12 + df[\"date\"].dt.month\n",
        "df[\"time_idx\"] -= df[\"time_idx\"].min()\n",
        "\n",
        "df['Avg sales by SKU'] = df.groupby([\"time_idx\", \"SKU id\"], observed=True).sales.transform(\"mean\")\n",
        "df['Avg sales by Warehouse'] = df.groupby([\"time_idx\", \"Warehouse id\"], observed=True).sales.transform(\"mean\")\n",
        "\n",
        "df.reset_index(level=0, drop=True, inplace=True)\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "from pytorch_forecasting.data import (\n",
        "    TimeSeriesDataSet,\n",
        "    GroupNormalizer\n",
        ")\n",
        "max_prediction_length = 6 # forecast 6 months\n",
        "max_encoder_length = 38  # use 38 months of history\n",
        "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    df[lambda x: x.time_idx <= training_cutoff],\n",
        "    time_idx=\"time_idx\",\n",
        "    target=\"sales\",\n",
        "    group_ids=[\"Warehouse id\", \"SKU id\", 'Region'],\n",
        "    min_encoder_length=0,  # allow predictions without history\n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"Warehouse id\", \"SKU id\", 'Region'],\n",
        "    static_reals=[],\n",
        "    time_varying_known_categoricals=['isHoli', 'isDiwali', 'isSankranti', 'isChristmas', 'isSummer', 'isWinter', \"month\"],\n",
        "    # group of categorical variables can be treated as \n",
        "    # one variable\n",
        "    variable_groups={\"special_months\": ['isHoli', 'isDiwali', 'isSankranti', 'isChristmas', 'isSummer', 'isWinter']},\n",
        "    time_varying_known_reals=[\n",
        "        \"time_idx\"\n",
        "    ],\n",
        "    time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=[\n",
        "        \"sales\",\n",
        "        \"sales previous month\",\n",
        "        \"diff sales 1\",\n",
        "        \"rolling mean sales 4\",\n",
        "        \"Avg sales by SKU\",\n",
        "        \"Avg sales by Warehouse\"\n",
        "    ],\n",
        "    target_normalizer=GroupNormalizer(\n",
        "        groups=[\"Warehouse id\", \"SKU id\", \"Region\"]\n",
        "    ),  # use softplus with beta=1.0 and normalize by group\n",
        "    add_relative_time_idx=True,  # add as feature\n",
        "    add_target_scales=True,  # add as feature\n",
        "    add_encoder_length=True,  # add as feature\n",
        "    allow_missing_timesteps=True\n",
        ")\n",
        "# create validation set (predict=True) which means to predict the\n",
        "# last max_prediction_length points in time for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(\n",
        "    training, df, predict=True, stop_randomization=True\n",
        ")\n",
        "# create dataloaders for model\n",
        "batch_size = 128\n",
        "train_dataloader = training.to_dataloader(\n",
        "    train=True, batch_size=batch_size, num_workers=0\n",
        ")\n",
        "val_dataloader = validation.to_dataloader(\n",
        "    train=False, batch_size=batch_size * 10, num_workers=0\n",
        ")\n",
        "\n",
        "# imports for training\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "# import dataset, network to train and metric to optimize\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
        "\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # log to tensorboard\n",
        "# create trainer\n",
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    gpus=[0],  # train on CPU, use gpus = [0] to run on GPU\n",
        "    gradient_clip_val=0.1,\n",
        "    limit_train_batches=30,  # running validation every 30 batches\n",
        "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "# initialise model\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=21,  # biggest influence network size\n",
        "    attention_head_size=1,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
        "    loss=QuantileLoss(),\n",
        "    log_interval=10,  # log example every 10 batches\n",
        "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
        ")\n",
        "\n",
        "res = trainer.tuner.lr_find(\n",
        "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n",
        ")\n",
        "\n",
        "print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "fig = res.plot(show=True, suggest=True)\n",
        "fig.show()\n",
        "\n",
        "# fit network\n",
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader\n",
        ")\n",
        "\n",
        "# from pytorch_forecasting.metrics import MAPE\n",
        "import torch\n",
        "\n",
        "# load the best model according to the validation loss (given that\n",
        "# we use early stopping, this is not necessarily the last epoch)\n",
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "\n",
        "# calculate mean absolute error on validation set\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader)\n",
        "\n",
        "print(predictions.shape)\n",
        "print(mape(predictions[:,0].cpu().detach().numpy(), actuals[:,0].cpu().detach().numpy()))\n",
        "print(mape(predictions[:,1].cpu().detach().numpy(), actuals[:,1].cpu().detach().numpy()))\n",
        "print(mape(predictions[:,2].cpu().detach().numpy(), actuals[:,2].cpu().detach().numpy()))\n",
        "print(mape(predictions[:,3].cpu().detach().numpy(), actuals[:,3].cpu().detach().numpy()))\n",
        "print(mape(predictions[:,4].cpu().detach().numpy(), actuals[:,4].cpu().detach().numpy()))\n",
        "print(mape(predictions[:,5].cpu().detach().numpy(), actuals[:,5].cpu().detach().numpy()))"
      ],
      "metadata": {
        "id": "pwF83HSo7-Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Prophet ######"
      ],
      "metadata": {
        "id": "wJQaBDMm_zIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version\n",
        "!pip install pyspark\n",
        "!pip install pyarrow==7.0.0\n",
        "!pip install fbprophet\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fbprophet import Prophet\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (10, 8)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "spark = SparkSession.builder.master('local').getOrCreate()\n",
        "df = pd.read_csv('train_data.csv')\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)\n",
        "df['sales'] = df['sales'].astype(float)\n",
        "\n",
        "sdf = spark.createDataFrame(df)\n",
        "sdf.select(['store_id']).groupBy('store_id').agg({'store_id':'count'}).show()\n",
        "sdf.createOrReplaceTempView(\"sales\")\n",
        "spark.sql(\"select store_id, count(*) from sales group by store_id order by store_id\").show()\n",
        "sql = 'SELECT store_id, date as ds, sum(sales) as y FROM sales GROUP BY store_id, ds ORDER BY store_id, ds'\n",
        "spark.sql(sql).show()\n",
        "sdf.rdd.getNumPartitions()\n",
        "store_part = (spark.sql( sql ).repartition(spark.sparkContext.defaultParallelism, ['store_id'])).cache()\n",
        "store_part.explain()\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from pyspark.sql.functions import current_date\n",
        "\n",
        "result_schema =StructType([\n",
        "  StructField('ds',TimestampType()),\n",
        "  StructField('store_id',IntegerType()),\n",
        "  StructField('y',DoubleType()),\n",
        "  StructField('yhat',DoubleType()),\n",
        "  StructField('yhat_upper',DoubleType()),\n",
        "  StructField('yhat_lower',DoubleType())\n",
        "  ])\n",
        "\n",
        "@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )\n",
        "def forecast_sales( store_pd ):\n",
        "\n",
        "  model = Prophet(interval_width=0.95,seasonality_mode = 'multiplicative', yearly_seasonality=True)\n",
        "\n",
        "  model.fit( store_pd )\n",
        "\n",
        "  future_pd = model.make_future_dataframe(\n",
        "    periods=5, \n",
        "    freq='m'\n",
        "    )\n",
        "  \n",
        "  forecast_pd = model.predict( future_pd )  \n",
        "  \n",
        "  f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
        "  \n",
        "  st_pd = store_pd[['ds','store_id','y']].set_index('ds')\n",
        "  \n",
        "  results_pd = f_pd.join( st_pd, how='left' )\n",
        "  results_pd.reset_index(level=0, inplace=True)\n",
        "  \n",
        "  results_pd['store_id'] = store_pd['store_id'].iloc[0]\n",
        "\n",
        "  return results_pd[ ['ds', 'store_id','y', 'yhat', 'yhat_upper', 'yhat_lower'] ]  "
      ],
      "metadata": {
        "id": "pfAZ1QWW_1Tr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}